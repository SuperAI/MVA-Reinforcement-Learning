{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_MVARL19_part1_last.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4P3WM-hVOPfo"
      },
      "source": [
        "# Reinforcement Learning in Finite MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E9_DLZvWQzhb",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wnzUJeyJOPfq"
      },
      "source": [
        "## MDPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RcWJSw_uOPfr",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')\n",
        "import numpy as np\n",
        "from scipy.special import softmax # for SARSA\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import math\n",
        "from random import *\n",
        "from cliffwalk import CliffWalk\n",
        "from test_env import ToyEnv1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ym-B_4HaOPfu"
      },
      "source": [
        "Setting up the environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rVR5qYoLOPfv",
        "colab": {}
      },
      "source": [
        "env = CliffWalk(proba_succ=0.98)\n",
        "\n",
        "####################################################################################\n",
        "# You probably want to test smaller enviroments before\n",
        "#env = ToyEnv1(gamma=0.99)\n",
        "####################################################################################\n",
        "\n",
        "# Useful attributes\n",
        "print(\"Set of states:\", env.states)\n",
        "print(\"Set of actions:\", env.actions)\n",
        "print(\"Number of states: \", env.Ns)\n",
        "print(\"Number of actions: \", env.Na)\n",
        "print(\"P has shape: \", env.P.shape)  # P[s, a, s'] = env.P[s, a, s']\n",
        "print(\"discount factor: \", env.gamma)\n",
        "print(\"\")\n",
        "\n",
        "# Usefult methods\n",
        "state = env.reset() # get initial state\n",
        "print(\"initial state: \", state)\n",
        "print(\"reward at (s=1, a=3,s'=2): \", env.reward_func(1,3,2))\n",
        "print(\"\")\n",
        "\n",
        "# A random policy\n",
        "policy = np.random.randint(env.Na, size = (env.Ns,))\n",
        "print(\"random policy = \", policy)\n",
        "\n",
        "# Interacting with the environment\n",
        "print(\"(s, a, s', r):\")\n",
        "for time in range(4):\n",
        "    action = policy[state]\n",
        "    next_state, reward, done, info = env.step(action)\n",
        "    print(state, action, next_state, reward)\n",
        "    if done:\n",
        "        break\n",
        "    state = next_state\n",
        "print(\"\")\n",
        "print(env.R.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AUlNvT3cOPfx"
      },
      "source": [
        "## Question 1: Value iteration\n",
        "1. Write a function applying the optimal Bellman operator on a provided Q function: $Q_1 = LQ_0, \\; Q_0\\in \\mathbb{R}^{S\\times A}$\n",
        "2. Write a function implementing Value Iteration (VI) with $\\infty$-norm stopping condition (reuse function implemented in 1)\n",
        "3. Evaluate the convergence of your estimate, i.e., plot the value $\\|Q_n - Q^\\star\\|_{\\infty} = \\max_{s,a} |Q_n(s,a) - Q^\\star(s,a)|$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R8TLRx6MOPfy",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 1\n",
        "# --------------\n",
        "def bellman_operator(Q0, Ns, Na, R, P, gamma):\n",
        "  Q1 = np.zeros((Ns,Na))\n",
        "  greedy_policy = []\n",
        "  for s in range(Ns):\n",
        "    for a in range(Na):\n",
        "      for sx in range(Ns):\n",
        "        Q1[s,a] = Q1[s,a] + R[s,a,sx]*P[s,a,sx]\n",
        "      for sn in range(Ns):\n",
        "        Q1[s,a] = Q1[s,a] + gamma*P[s,a,sn]*np.max(Q0[sn,:])\n",
        "    greedy_policy.append(np.argmax(Q1[s,:]))\n",
        "  return Q1, greedy_policy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jj65cQk5OPf0",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 2\n",
        "# --------------\n",
        "def value_iteration(Q0, env, epsilon=1e-5):\n",
        "    Q_history = [Q0]\n",
        "    Ns = Q0.shape[0]\n",
        "    Na =Q0.shape[1]\n",
        "    Q,greedy_policy = bellman_operator(Q0, Ns, Na, env.R, env.P, env.gamma)\n",
        "    Q_history.append(Q)\n",
        "    while np.amax(Q-Q0) > epsilon*np.amax(Q0):\n",
        "      Q0 = Q\n",
        "      Q,greedy_policy = bellman_operator(Q0, Ns, Na, env.R, env.P, env.gamma)\n",
        "      Q_history.append(Q)\n",
        "    return Q, greedy_policy, Q_history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W_lBe6q6OPf2",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 3\n",
        "# --------------\n",
        "with open(\"./mvarl_hands_on/data/Q_opts.json\", \"r\") as fp:\n",
        "    Qopts = json.load(fp)\n",
        "Qstar = Qopts[\"{}_{}\".format(type(env).__name__,env.gamma)]\n",
        "Qstar = np.array(Qstar)\n",
        "Q0 = np.zeros((Qstar.shape[0],Qstar.shape[1]))\n",
        "Q, greedy_policy, Q_history = value_iteration(Q0, env, epsilon=1e-5)\n",
        "norm_values = [np.amax(abs(Q_history[i]-Qstar)) for i in range(len(Q_history))]\n",
        "\n",
        "plt.plot(norm_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Q-learning: Convergence of Q\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FbzFs3tDvcJy",
        "colab": {}
      },
      "source": [
        "state = env.reset()\n",
        "env.render()\n",
        "for i in range(50):\n",
        "    action = greedy_policy[state]\n",
        "    state, reward, done, _ = env.step(action)\n",
        "    env.render()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "azfXmRzZOPf4"
      },
      "source": [
        "## Question 2: Q learning\n",
        "Q learning is a model-free algorithm for estimating the optimal Q-function online.\n",
        "It is an off-policy algorithm since the samples are collected with a policy that is (potentially) not the one associated to the estimated Q-function.\n",
        "\n",
        "1. Implement Q learning with $\\epsilon$-greedy exploration.\n",
        "  - Plot the error in Q-functions over iterations\n",
        "  - Plot the sum of rewards as a function of iteration\n",
        "\n",
        "\n",
        "$\\epsilon$-greedy policy:\n",
        "$$\n",
        "\\pi(s) = \\begin{cases}\n",
        "\\max_a Q(s,a) & \\text{w.p.} \\epsilon\\\\\n",
        "\\text{random action} & \\text{w.p.} 1- \\epsilon\n",
        "\\end{cases}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNgkc6A2iaIv",
        "colab_type": "text"
      },
      "source": [
        "We choose $\\epsilon = 0.5$ and the learning $rate = \\frac{1}{{\\#(a_{t},s_{t})}^{0.2}}$ where $\\#(a_{t},s_{t})$ is the number of time the agent is in the state $s_{t}$ with the action $a_{t}$. In practice, this learning-rate works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rtszdNuZ1cH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ---------------------------\n",
        "# Q-Learning\n",
        "# ---------------------------\n",
        "# suggested interface\n",
        "# you can change it!\n",
        "class QLearning:\n",
        "    \"\"\"\n",
        "    Q learning with epsilon-greedy exploration\n",
        "    \"\"\"\n",
        "    def __init__(self,n,tmax,gamma,epsilon,env):\n",
        "      self.n = n\n",
        "      self.tmax = tmax\n",
        "      self.gamma = gamma\n",
        "      self.epsilon = epsilon\n",
        "      self.env = env\n",
        "      self.Q = np.zeros((self.env.Ns,self.env.Na))\n",
        "      self.V = np.zeros((self.n,self.env.Ns))\n",
        "      self.cumulative_reward = np.zeros(self.n)\n",
        "      self.count = np.zeros(((self.env.Ns,self.env.Na)), dtype=int)\n",
        "    \n",
        "    def sample_action(self, state):\n",
        "      x = np.random.binomial(1,self.epsilon)\n",
        "      return x*np.argmax(self.Q[state,:]) + (1-x)*randint(0,self.env.Na-1)\n",
        "    \n",
        "    def update(self, state, action, next_state, reward):\n",
        "      delta = reward + self.gamma*np.max(self.Q[next_state,:]) - self.Q[state,action]\n",
        "      self.Q[state,action] = self.Q[state,action] + 1/(self.count[state,action]**0.2)*delta\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChsBb3rSZ5mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q0 = np.zeros((env.Ns, env.Na))\n",
        "# Use the previous code to verify the correctness of q learning\n",
        "Q_opt, pi_opt, Qhist_opt = value_iteration(Q0, env, epsilon=1e-8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CKTc5nWIOPf6",
        "colab": {}
      },
      "source": [
        "# --------------\n",
        "# Point 1\n",
        "# --------------\n",
        "# Number of Q learning steps\n",
        "max_steps = int(1e5)  \n",
        "#max_steps = 10\n",
        "\n",
        "# main algorithmic loop\n",
        "ql = QLearning(n=max_steps, tmax=50, gamma=env.gamma, epsilon=0.5, env=env)\n",
        "norm_values = []\n",
        "t = 0\n",
        "tmax = 50\n",
        "rewardv = np.zeros((max_steps,tmax), dtype=int)\n",
        "gammas = np.array([env.gamma**k for k in range(tmax)])\n",
        "for i in range(max_steps):\n",
        "  state = env.reset()\n",
        "  for t in range(tmax):\n",
        "    action = ql.sample_action(state)\n",
        "    ql.count[state,action] = ql.count[state,action] + 1\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    rewardv[i,t] = reward\n",
        "    ql.update(state, action, observation, reward)\n",
        "    state = observation\n",
        "  norm_values.append(np.abs(ql.Q - Q_opt).mean())\n",
        "  ql.V[i,:] = np.max(ql.Q,axis=1)\n",
        "  ql.cumulative_reward[i] = ql.cumulative_reward[i-1] + rewardv[i,:].dot(gammas)\n",
        "\n",
        "\n",
        "    \n",
        "print(env.render())\n",
        "print(\"optimal policy: \", pi_opt)\n",
        "greedy_policy = np.argmax(ql.Q, axis=1)\n",
        "print(\"est policy:\", greedy_policy)\n",
        "\n",
        "\n",
        "plt.plot(norm_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Error')\n",
        "plt.title(\"Q-learning: Convergence of Q\")\n",
        "\n",
        "# how confident are you in the performance of the algorithm? maybe a single run is not enough"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6a0lL7lGydh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(ql.cumulative_reward)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Cumulative Reward')\n",
        "plt.title(\"Q-learning: Convergence of Q\")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}